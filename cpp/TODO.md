# Things To Do

## Higher Priority

- using stochastic search:
  - implement batching for stochastic search. duplicate code is fine.
  - Switch repl to use stochastic search implementations.
  - benchmark for various orders.
  - If always better than backtracking, just stop using backtracking. Consider even deleting the batching implementation? Or just removing it from the library api. The backtracking implementation can just be used as a reference for a more complicated deductive reasoning solver when deductive reasoning can't further progress.
    - If way worse than backtracking for some orders, decide what best to do then.

- compare intrinsic statistical properties of grids generated by backtracking and stochastic search. If there is no difference, just deprecate the backtracking method? hm. It's possible it could be better at solving grids that aren't solvable by pure deductive reasoning techniques...

- after experimenting with different stochastic implementations, try implementing an opencl program. The minstd_rand rng is very simple to implement. I think the stochastic algorithm is data-parallelizable.

- update the print function to work for large grids.

- experiment with the option of making each order be its own dynamic library.
- try making Order an enum
  - see if it can improve switch case cover detection.
  - this may also make it possible to statically enforce contracts about orders for the non-template wrapper functions. If so, I will probably neither need to assert that the order is compiled, nor write vacuous default branches for their switch statements.
  - if this works out, make sure to update all the contract docs and remove relevant assertions.
- consider giving the callback in batch a dedicated mutex, or no mutex at all and leaving it up to the caller. need to consider how likely it is that the bulk of a callback will need synchronization.
- make a buffering adapter for batch callbacks.
  - use destructor to flush
- in the repl config, consider making some fields per-order. max_dead_ends is a good candidate. verbosity might also be useful, but I'm not sure if it would be surprising in a bad way. should be fine as long as the current values are printed when switching between orders.
- the opcount diagnostics shouldn't be taken seriously. opcount itself currently isn't very representative of effort expended since it doesn't count iterations of the try_val loop.

- make some grid things for binary and text serdes.
  - this can be useful for gathering up a dataset of order-5 grids for future experimentation and benchmarking/testing of the non-generation parts of this library (scramble, canonicalize).

- adapt canonicalization to work on puzzles (incomplete grids).
  - this would allow checking if puzzles are equivalent.

- Go back and try the old canonicalization by rel row prob, but break ties by doing some brute force: try each tied permutation and valuate it according to some reduction of how it pushes rarer rel counts to the top left. Just be careful to shift to get rid of the main diagonal seam.
  - If there are multiple puddles of ties, the resolution of a puddle shouldn't depend on the resolution of any other puddle- only on the non-tied rows/columns. A consequence of this is that this resolution algorithm will not work if there are no non-tied rows/columns.

1. Consider: The current relabelling canonicalization method may have a big weakness: I think ties can be easily crafted: consider the "Most Canonical" solution grid- it would be all ties. How can this be addressed? (Or perhaps the "Most Canonical" grid is the only weakness?)
    - Break ties by designing a way to give symbols that frequently cohabit atoms label-values that are closer together in value:
    - The cohabitation table can be tiled; a coordinate's vertical or horizontal distance (these are the same, since relabelling moves both the col and row together) from the closest tiling of the main diagonal represents the distance of the labels from each other.
    - For each label, make an array where each index represents another label, and the value is an object containing the left and right distances between them, and the cohabitation count.
    - Hm. So far this seems to suggest a hill-climbing / brute-force-type solution...
    - OR... make an empty commit saying that a solution was implemented using the [Ostritch Alorithm](https://en.wikipedia.org/wiki/Ostrich_algorithm)

- some diagnostics to try rendering:
  - A scatter chart showing max-dead-ends vs. num operations. (only caring about success results)
  - a bar graph where each bar counts the number of ResultViews that had a progress (or a furthest coord with a non-zero backtrack count) within the range for that bar's "bin". (to see "how far" aborted generations usually get).
  - comparing the average heatmaps of aborted vs successful generations.
    - See if there are clear differences/patterns in where they usually spike in backtracking. Perhaps this can be used to inform more sophisticated thresholds for each genpath.

- ? Refactor names to use terminology suitable for more than just 2 dimensions? Ex. in 2D: row -> `d0i` (as in "dimension-zero index"), col -> `d1i`. But doing so would imply that I'm going to support multiple dimensions... and that's a huge can of worms.

- (?) Change canonicalization to not use templates? benchmark and compare
- Decide what interfaces to support:
  - Probably best to start with just readline and a CLI
    - For CLI util libraries, look into using
      - repl
        - https://github.com/daniele77/cli
      - command
        - https://github.com/docopt/docopt.cpp
        - https://github.com/CLIUtils/CLI11
        - http://tclap.sourceforge.net/manual.html
        - https://github.com/Taywee/args
  - Can look into ncurses in the future? Or look into options for TUI libraries?
  - A web interface would be really nice.
- C++20
  - `using enum`. Might want to wait for CLANG to support?
- CLI
  - implement `-h` and `--help` CLI argument.
  - give a red message when trying to continue and nothing is left to be found.

## Interesting Questions for Further Research

- Can the scramble-invariant property analysis of a grid be used to efficiently estimate the difficulty of a puzzle? I believe (and hope) there is potential that the answer is yes. If it is the case, how? And what would the accuracy of the estimation be (error distribution)?
  - If so, could it be used to _create_ probably difficult puzzles?

- Lossless Compression: What specific pattern of cells can always be removed from a solution while guaranteeing that the result is _trivial_ to restore to the full solution?
  - One option:
    - The blocks along a diagonal / shifted diagonal
    - one cell in each remaining block?
  - Usefulness:
    - To reduce unnecessary bytes sent over network? simple compressed storage?